{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------import nécessaires\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire liens et nom de catégories pour les mettre dans un dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création du dictionnaire de catégories et du driver chrome\n",
    "categories = {}\n",
    "driver = webdriver.Chrome()\n",
    "# Ouvrir une page web\n",
    "driver.get(\"https://techcrunch.com/category/\")\n",
    "d\n",
    "#attente 3s le temps que le popup cookies se charge\n",
    "wait = WebDriverWait(driver, 3)\n",
    "scroll_button = wait.until(EC.element_to_be_clickable((By.ID, 'scroll-down-btn')))\n",
    "\n",
    "# Cliquer sur le bouton pour accepter les cookies/fermer le popup\n",
    "scroll_button.click()\n",
    "\n",
    "cookie_button = wait.until(EC.element_to_be_clickable((By.NAME, 'reject')))\n",
    "\n",
    "# Cliquer sur le bouton pour accepter les cookies/fermer le popup\n",
    "cookie_button.click()\n",
    "\n",
    "# Attendre que la page soit entièrement chargée\n",
    "WebDriverWait(driver, 60).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "# Obtenez le code source de la page\n",
    "html = driver.page_source\n",
    "\n",
    "# Fermer le navigateur une fois que terminé\n",
    "driver.quit()\n",
    "\n",
    "#utilisation de beautifulSoup pour extraire les liens et noms des catégories et faire un premier remplissage du dictionnaire\n",
    "soup = BeautifulSoup(html, 'html')\n",
    "ul = soup.find('ul',class_='menu not-found__menu')\n",
    "lis = ul.find_all('li',class_='menu__item')\n",
    "for li in lis:\n",
    "    a = li.find('a')\n",
    "    url = 'https://techcrunch.com'+a['href']\n",
    "    category = a.text\n",
    "    categories[category] = {'url': url, 'desc': None, 'articles':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI': {'url': 'https://techcrunch.com/category/artificial-intelligence/', 'desc': None, 'articles': []}, 'Apps': {'url': 'https://techcrunch.com/category/apps/', 'desc': None, 'articles': []}, 'Biotech & Health': {'url': 'https://techcrunch.com/category/biotech-health/', 'desc': None, 'articles': []}, 'Climate': {'url': 'https://techcrunch.com/category/climate/', 'desc': None, 'articles': []}, 'Commerce': {'url': 'https://techcrunch.com/category/commerce/', 'desc': None, 'articles': []}, 'Crypto': {'url': 'https://techcrunch.com/category/cryptocurrency/', 'desc': None, 'articles': []}, 'Enterprise': {'url': 'https://techcrunch.com/category/enterprise/', 'desc': None, 'articles': []}, 'Fintech': {'url': 'https://techcrunch.com/category/fintech/', 'desc': None, 'articles': []}, 'Gadgets': {'url': 'https://techcrunch.com/category/gadgets/', 'desc': None, 'articles': []}, 'Gaming': {'url': 'https://techcrunch.com/category/gaming/', 'desc': None, 'articles': []}, 'Government & Policy': {'url': 'https://techcrunch.com/category/government-policy/', 'desc': None, 'articles': []}, 'Hardware': {'url': 'https://techcrunch.com/category/hardware/', 'desc': None, 'articles': []}, 'Media & Entertainment': {'url': 'https://techcrunch.com/category/media-entertainment/', 'desc': None, 'articles': []}, 'Privacy': {'url': 'https://techcrunch.com/category/privacy/', 'desc': None, 'articles': []}, 'Robotics': {'url': 'https://techcrunch.com/category/robotics/', 'desc': None, 'articles': []}, 'Security': {'url': 'https://techcrunch.com/category/security/', 'desc': None, 'articles': []}, 'Social': {'url': 'https://techcrunch.com/category/social/', 'desc': None, 'articles': []}, 'Space': {'url': 'https://techcrunch.com/category/space/', 'desc': None, 'articles': []}, 'Startups': {'url': 'https://techcrunch.com/category/startups/', 'desc': None, 'articles': []}, 'TC': {'url': 'https://techcrunch.com/category/tc/', 'desc': None, 'articles': []}, 'Transportation': {'url': 'https://techcrunch.com/category/transportation/', 'desc': None, 'articles': []}, 'Venture': {'url': 'https://techcrunch.com/category/venture/', 'desc': None, 'articles': []}}\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_attributes(url_page,current_driver):\n",
    "    \"\"\"\n",
    "    créé un dictionnaire article, le remplie et le retourne\n",
    "    Retourne un dictionnaire article.\n",
    "    \"\"\"\n",
    "    current_driver.get(url_article)\n",
    "    WebDriverWait(current_driver, 60).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    # Obtenir le code source de la page\n",
    "    html_article = current_driver.page_source\n",
    "    soup_article = BeautifulSoup(html_article, 'html')\n",
    "#-----------------------création du dictionnaire d'article avec ses attributs\n",
    "    article = {\n",
    "        'title' : soup_article.find('h1',class_='article__title').text,\n",
    "        'author' : soup_article.find('a', href=lambda href: href and \"author\" in href).text,\n",
    "        'creation_date' : soup_article.find('time',class_='full-date-time').text,\n",
    "        'content' : soup_article.find('div',class_='article-content').get_text(separator=' ', strip=True),\n",
    "        'url' : url_page,\n",
    "        'tags' : soup_article.find('ul',class_='menu article__tags__menu').get_text(separator=', ', strip=True),\n",
    "        'comments': ['No one seems to have shared their thoughts on this topic yet']\n",
    "    }\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extraire les attributs des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "tv0FTOB3toDI",
    "outputId": "8ce21f3b-1e3f-4a72-80dc-f53e0a420317"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#variable pour ne cliquer sur le popup des cookies une seule fois\n",
    "handle_popup_cookies = True\n",
    "for category in categories:\n",
    "    url_current = categories[category]['url']\n",
    "    # Ouvrir une page web\n",
    "    driver.get(url_current)\n",
    "#----------------------------------------Gestion popup cookies (1 seul fois)\n",
    "    if(handle_popup_cookies):\n",
    "        wait = WebDriverWait(driver, 3)\n",
    "        scroll_button = wait.until(EC.element_to_be_clickable((By.ID, 'scroll-down-btn')))\n",
    "        \n",
    "        # Cliquer sur le bouton pour accepter les cookies/fermer le popup\n",
    "        scroll_button.click()\n",
    "        \n",
    "        cookie_button = wait.until(EC.element_to_be_clickable((By.NAME, 'reject')))\n",
    "        \n",
    "        # Cliquer sur le bouton pour accepter les cookies/fermer le popup\n",
    "        cookie_button.click()\n",
    "    \n",
    "        handle_popup_cookies = False\n",
    "        \n",
    "#--------------------------------------------------------------------------Scrapping des données des articles    \n",
    "    # Attendre que la page soit entièrement chargée\n",
    "    WebDriverWait(driver, 60).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "    for i in range(3):\n",
    "        load_more = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'load-more ')))\n",
    "        \n",
    "        # Cliquer sur le bouton pour accepter les cookies/fermer le popup\n",
    "        load_more.click()\n",
    "\n",
    "    # Obtenir le code source de la page\n",
    "    html_category = driver.page_source\n",
    "#------------------------------------------------traitement avec beautifulSoup\n",
    "    soup = BeautifulSoup(html_category, 'html')\n",
    "    test_None = soup.find('div',class_='river__description')\n",
    "#----------------------certaines catégories n'ont pas de description comme tcl\n",
    "    if test_None is not None:\n",
    "        category_description = test_None.text\n",
    "        categories[category]['desc'] = category_description\n",
    "#----------------------extraire les champs des articles\n",
    "    articles = soup.find_all('article')\n",
    "    for article in articles:\n",
    "        url_article = 'https://techcrunch.com'+article.find('a',class_ = 'post-block__title__link')['href']\n",
    "#---------------------fonction définie plus haut\n",
    "        X = get_article_attributes(url_article,driver)\n",
    "        categories[category]['articles'].append(X)\n",
    "# Fermer le navigateur une fois terminé\n",
    "driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation du dictionnaire en fichiers json pour un stockage plus efficace\n",
    "with open('articles.json','w') as fichier_json:\n",
    "    json.dump(categories,fichier_json,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_time_date(chaine_date_heure):\n",
    "    \"\"\"\n",
    "    Faute de temps, j'ai eu quelques problèmes pour supprimer les caractères \\u2022 donc j'ai utilisé une méthode (CTRL+F) pour remplacer tous les caractères \n",
    "    \\u2022 par --- ce qui est beaucoup plus simple à traiter.\n",
    "    \n",
    "    la fonction prend en entrée une chaine date et heure et divise en deux attributs date et heure.\n",
    "    retourne deux chaines date_sql et heure_sql\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        # diviser le string en utilisant '---' \n",
    "        time_str, date_str = chaine_date_heure.split(\"---\", 1)\n",
    "\n",
    "        time_parts = time_str.split(\" \")\n",
    "        if len(time_parts) != 3:\n",
    "            raise ValueError(\"Time format is not valid.\")\n",
    "\n",
    "        hour_min, am_pm, timezone = time_parts\n",
    "\n",
    "        time_24hr = datetime.strptime(hour_min + ' ' + am_pm, '%I:%M %p').strftime('%H:%M')\n",
    "        heure_sql = time_24hr + \" \" + timezone\n",
    "\n",
    "        date_str_cleaned = re.sub(r'^\\d+', '', date_str).strip()\n",
    "        \n",
    "        date_obj = datetime.strptime(date_str_cleaned, \"%B %d, %Y\")\n",
    "\n",
    "        # Formate la date en \"YYYY-MM-DD\"\n",
    "        date_sql = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        return date_sql, heure_sql\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        raise ValueError(f\"The format of the date and time string is not valid: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['très long texte qui pourrait être un grand paragraphe ou un document étendu, utilisé pour des tests de division.']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diviser_texte_dynamiquement(texte, taille_max_par_partie=3500):\n",
    "    \"\"\"\n",
    "    Divise un texte en plusieurs parties, chaque partie ayant une taille maximale spécifiée.\n",
    "    Retourne une liste des parties.\n",
    "    Cette fonction a été créé car dans oracle, une commande a une taille maximal de 4000 caractères\n",
    "    \"\"\"\n",
    "    parties = []\n",
    "    debut = 0\n",
    "\n",
    "    while debut < len(texte):\n",
    "        fin = debut + taille_max_par_partie\n",
    "\n",
    "        # Ajuster le point de division pour ne pas couper en plein milieu d'un mot\n",
    "        while fin < len(texte) and texte[fin] not in \" \\t\\n\":\n",
    "            fin += 1\n",
    "\n",
    "        parties.append(texte[debut:fin])\n",
    "        debut = fin\n",
    "\n",
    "    return parties\n",
    "\n",
    "# Exemple d'utilisation\n",
    "texte_long = \"très long texte qui pourrait être un grand paragraphe ou un document étendu, utilisé pour des tests de division.\"\n",
    "parties_dynamiques = diviser_texte_dynamiquement(texte_long)\n",
    "\n",
    "parties_dynamiques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "création des fichiers insertions et csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Noms des fichiers\n",
    "input_json_filename = 'articles.json'  # Nom de votre fichier JSON\n",
    "output_sql_categories_filename = 'insert_categories.sql'\n",
    "output_sql_articles_filename = 'insert_articles.sql'\n",
    "output_csv_categories_filename = 'insert_categories.csv'\n",
    "output_csv_articles_filename = 'insert_articles.csv'\n",
    "\n",
    "# Ouvrir le fichier JSON pour lecture\n",
    "with open(input_json_filename, 'r', encoding='utf-8') as json_file:\n",
    "    categories = json.load(json_file)\n",
    "\n",
    "# Ouvrir les fichiers SQL et CSV pour écriture\n",
    "with open(output_sql_categories_filename, 'w', encoding='utf-8') as sql_categories_file, \\\n",
    "     open(output_sql_articles_filename, 'w', encoding='utf-8') as sql_articles_file, \\\n",
    "     open(output_csv_categories_filename, 'w', newline='', encoding='utf-8') as csv_categories_file, \\\n",
    "     open(output_csv_articles_filename, 'w', newline='', encoding='utf-8') as csv_articles_file:\n",
    "\n",
    "    # CSV Writers\n",
    "    csv_categories_writer = csv.writer(csv_categories_file)\n",
    "    csv_articles_writer = csv.writer(csv_articles_file)\n",
    "\n",
    "    # Écrire les en-têtes pour chaque fichier CSV\n",
    "    csv_categories_writer.writerow(['ID_Category', 'Name_Category','URL_Category','Description'])\n",
    "    csv_articles_writer.writerow(['ID_article', 'ID_Category', 'Title','Author', 'CreationDate','Creation_Time', 'Content_Article','URL_Article','Tags'])\n",
    "\n",
    "#------------initialiser compteurs de catégorie et articles pour faire les références par la suite\n",
    "    num_cat = 0\n",
    "    numéro_article = 0\n",
    "    for category, item in categories.items():\n",
    "        num_cat += 1\n",
    "        \n",
    "        # Referance et description corrigé\n",
    "        ref_category = f\"Cat-{num_cat:02}\"\n",
    "        if categories[category]['desc'] is not None:\n",
    "            Description_corrected = categories[category]['desc'].replace(\"'\", \"''\").replace(\"&\",\"and\")\n",
    "            category_corrected = category.replace(\"&\",\"and\")\n",
    "\n",
    "        # SQL Insert\n",
    "        insert_categories = f\"INSERT INTO Article_Category (ID_Category,Name_Category,URL_Category,Description_) VALUES ('{ref_category}', '{category_corrected}','{categories[category]['url']}','{Description_corrected}');\\n\"\n",
    "        sql_categories_file.write(insert_categories)\n",
    "\n",
    "        # CSV Insert\n",
    "        csv_categories_writer.writerow([ref_category, category,categories[category]['url'],categories[category]['desc']])\n",
    "#--------------------------boucler sur les articles pour traitement       \n",
    "        for article in categories[category]['articles']:\n",
    "            numéro_article += 1\n",
    "\n",
    "#-------------------------diviser le champs date_temps en deux champs date et temps\n",
    "            date, time = divide_time_date(article['creation_date'])\n",
    "#-------------------------remplacer les & par and pour oracle, et ' par '' pour oracle également.\n",
    "            for key in article:\n",
    "                if key != 'comments':\n",
    "                    article[key] = article[key].replace(\"---\",\" \").replace(\"'\", \"''\").replace(\"&\",\"and\")\n",
    "            # Ref et text corrigé\n",
    "            ref_article = f\"Art-{numéro_article:02}\"\n",
    "            text_corrected = article['content']\n",
    "\n",
    "            # SQL Insert\n",
    "            insert_articles = f\"INSERT INTO Article (ID_Article, ID_Category, Title, Author,Creation_Date,Creation_Time,URL_Article,Tags)VALUES ('{ref_article}', '{ref_category}', '{article['title']}', '{article['author']}', TO_DATE('{date}', 'YYYY-MM-DD'), '{time}', '{article['url']}','{article['tags']}');\\n\"\n",
    "            sql_articles_file.write(insert_articles)\n",
    "            text_parts = diviser_texte_dynamiquement(text_corrected)\n",
    "#-----------------------diviser le champ texte en plusieurs parties dynamiquement tel que chaque partie à nombre_char < 3500 car en oracle la limite\n",
    "#-----------------------de caractères par commande est 4000\n",
    "            for i in range(len(text_parts)):\n",
    "                if i == 0:\n",
    "                    insert_articles = f\"UPDATE Article SET Content_Article ='{text_parts[i]}' WHERE ID_Article = '{ref_article}';\\n\"\n",
    "                else:\n",
    "                    insert_articles = f\"UPDATE Article SET Content_Article = Content_Article || '{text_parts[i]}' WHERE ID_Article = '{ref_article}';\\n\"\n",
    "                sql_articles_file.write(insert_articles) \n",
    "\n",
    "\n",
    "            # CSV Insert\n",
    "            csv_articles_writer.writerow([ref_article, ref_category, article['title'], article['author'], date, time, text_corrected, article['url'],article['tags']])\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
